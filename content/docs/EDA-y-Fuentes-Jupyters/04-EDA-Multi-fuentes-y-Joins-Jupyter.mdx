---
title: "EDA Multi fuentes y Joins Jupyter"
---

# Tarea 4: EDA Multi-fuentes y Joins - Fill in the Blanks
## G1: Joaqu√≠n Batista, Milagros Cancela, Valent√≠n Rodr√≠guez, Alexia Aurrecoechea, Nahuel L√≥pez
### UT1: An√°lisis Exploratorio de Datos | Pr√°ctica Guiada

# UT1: An√°lisis Exploratorio de Datos | Pr√°ctica Guiada

## üéØ Objetivos B√°sicos
- Aprender a integrar datos de m√∫ltiples fuentes  
- Dominar los diferentes tipos de joins con pandas  
- Realizar an√°lisis agregados con `groupby`  
- Crear reportes consolidados de datos integrados  

---

## üìã Lo que necesitas saber ANTES de empezar
- Conceptos b√°sicos de pandas (`read_csv`, `DataFrame`)  
- Idea general de bases de datos relacionales  
- Conceptos b√°sicos de joins (`inner`, `left`, `right`, `outer`)  

---

## üîß Paso 1: Setup Inicial
### üìã CONTEXTO DE NEGOCIO (CRISP-DM: Business Understanding)

### üîó Referencias oficiales
- [NYC Taxi Dataset](https://www.nyc.gov/assets/tlc/downloads/pdf/trip_record_data_user_guide.pdf)  
- [NYC Taxi Zones](https://www.nyc.gov/assets/tlc/downloads/pdf/taxi_zone_lookup.pdf)  
- [Pandas merge documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html)  

---

## üöï Caso de negocio
- **Problema:** La comisi√≥n de taxis de NYC necesita an√°lisis en tiempo real de **3+ millones de viajes mensuales**.  
- **Objetivo:** Integrar datos oficiales completos para entender patrones metropolitanos reales.  
- **Escala:** ~3M viajes, 265 zonas, m√∫ltiples boroughs, datos en tiempo real.  
- **Variables:** Viajes oficiales NYC (enero 2023), zonas geogr√°ficas completas, eventos calendario.  
- **Valor para el negocio:** Decisiones basadas en datos reales a escala metropolitana.  

```python
# Importar librer√≠as que vamos a usar
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sqlite3
from pathlib import Path

# Configurar visualizaciones
plt.style.use('default')
sns.set_palette('husl')
plt.rcParams['figure.figsize'] = (10, 6)

print("‚úÖ Setup completo para an√°lisis multi-fuentes!")
```

```python
!pip install pyarrow
```

```python
# === CARGAR DATOS DE M√öLTIPLES FUENTES ===

# 1. Cargar datos de viajes desde Parquet (Dataset oficial completo NYC)
print("Cargando datos oficiales de NYC Taxi (dataset completo)...")
trips_url = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet"

# Cargar dataset oficial (~3M registros de enero 2023)
trips = pd.read_parquet(trips_url)  # funci√≥n para leer archivos .parquet (m√°s eficiente que CSV)

print(f"   Viajes cargados: {trips.shape[0]:,} filas, {trips.shape[1]} columnas")
print(f"   Columnas: {list(trips.columns)}")
print(f"   Per√≠odo: {trips['tpep_pickup_datetime'].min()} a {trips['tpep_pickup_datetime'].max()}")
print(f"   Tama√±o en memoria: {trips.memory_usage(deep=True).sum() / 1024**2:.1f} MB")

# 2. Cargar datos de zonas desde CSV (Dataset oficial completo)
print("\nCargando datos oficiales de zonas NYC...")
zones_url = "https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv"
zones = pd.read_csv(zones_url)  # funci√≥n est√°ndar para archivos CSV

print(f"   Zonas cargadas: {zones.shape[0]} filas, {zones.shape[1]} columnas")
print(f"   Columnas: {list(zones.columns)}")
print(f"   Boroughs √∫nicos: {zones['Borough'].unique()}")

# 3. Cargar calendario de eventos desde JSON
print("\nCargando datos de calendario de eventos...")
calendar_url = "https://juanfkurucz.com/ucu-id/ut1/data/calendar.json"
calendar = pd.read_json(calendar_url)  # funci√≥n para archivos JSON
calendar['date'] = pd.to_datetime(calendar['date']).dt.date  # convertir strings a fechas, luego extraer solo la fecha

print(f"   Eventos calendario: {calendar.shape[0]} filas")
print(f"   Columnas: {list(calendar.columns)}")

# 4. Mostrar primeras filas de cada dataset
print("\nVISTA PREVIA DE DATOS:")
print("\n--- TRIPS ---")
print(trips.head())  # m√©todo para mostrar primeras filas de un DataFrame
print("\n--- ZONES ---")
print(zones.head())  # mismo m√©todo para ver estructura de datos
print("\n--- CALENDAR ---")
print(calendar.head())  # revisar formato de los eventos
```

```python
# === NORMALIZAR Y PREPARAR DATOS PARA JOINS ===

# 1. Estandarizar nombres de columnas
print("Normalizando nombres de columnas...")
trips.columns = trips.columns.str.lower()  # convertir todas las columnas a min√∫scilas
zones.columns = zones.columns.str.lower()  # misma transformaci√≥n para consistencia

print(f"   Trips columnas: {list(trips.columns)}")
print(f"   Zones columnas: {list(zones.columns)}")

# 2. Crear columna de fecha para el join con calendario
trips['pickup_date'] = trips['tpep_pickup_datetime'].dt.date  # extraer solo la fecha (sin hora) de la columna datetime

print(f"   Columna pickup_date creada")
print(f"   Rango de fechas: {trips['pickup_date'].min()} a {trips['pickup_date'].max()}")

# 3. Verificar tipos de datos para joins
print("\nVERIFICACI√ìN DE TIPOS PARA JOINS:")
print(f"   trips['pulocationid'] tipo: {trips['pulocationid'].dtype}")
print(f"   zones['locationid'] tipo: {zones['locationid'].dtype}")
print(f"   trips['pickup_date'] tipo: {type(trips['pickup_date'].iloc[0])}")
print(f"   calendar['date'] tipo: {type(calendar['date'].iloc[0])}")

# 4. Optimizaci√≥n para datasets grandes (~3M registros)
print("\nOPTIMIZACI√ìN PARA DATASETS GRANDES:")
initial_memory = trips.memory_usage(deep=True).sum() / 1024**2
print(f"   Memoria inicial: {initial_memory:.1f} MB")

# Optimizar tipos de datos para 3+ millones de registros
print("   Optimizando tipos de datos para 3M+ registros...")

# Limpiar valores nulos antes de convertir tipos
print("   Limpiando valores nulos antes de optimizaci√≥n...")
trips['passenger_count'] = trips['passenger_count'].fillna(1)  # m√©todo para rellenar valores nulos con un valor espec√≠fico
trips = trips.dropna(subset=['pulocationid', 'dolocationid'])  # eliminar filas cr√≠ticas sin ubicaci√≥n (necesarias para joins)

# Convertir tipos despu√©s de limpiar
trips['pulocationid'] = trips['pulocationid'].astype('int16')
trips['dolocationid'] = trips['dolocationid'].astype('int16')
trips['passenger_count'] = trips['passenger_count'].astype('int8')
zones['locationid'] = zones['locationid'].astype('int16')

print(f"   Registros despu√©s de limpieza: {len(trips):,}")

optimized_memory = trips.memory_usage(deep=True).sum() / 1024**2
savings = ((initial_memory - optimized_memory) / initial_memory * 100)

print(f"   Memoria optimizada: {optimized_memory:.1f} MB")
print(f"   Ahorro de memoria: {savings:.1f}%")

# 5. Revisar datos faltantes antes de joins
print("\nDATOS FALTANTES ANTES DE JOINS:")
print("Trips (top 5 columnas con m√°s nulos):")
trips_nulls = trips.isna().sum().sort_values(ascending=False).head()  # m√©todo para detectar valores nulos, sumar y ordenar
print(trips_nulls)

print("\nZones:")
zones_nulls = zones.isna().sum()  # revisar si hay valores faltantes en lookup table
print(zones_nulls)

print("\nCalendar:")
calendar_nulls = calendar.isna().sum()  # verificar integridad del calendario de eventos
print(calendar_nulls)

# An√°lisis de calidad de datos
print("\nAN√ÅLISIS DE CALIDAD:")
total_trips = len(trips)
print(f"   Total de viajes: {total_trips:,}")
print(f"   Viajes sin pickup location: {trips['pulocationid'].isna().sum():,}")
print(f"   Viajes sin dropoff location: {trips['dolocationid'].isna().sum():,}")
print(f"   Viajes sin passenger_count: {trips['passenger_count'].isna().sum():,}")

# Estrategias de limpieza recomendadas
print("\nESTRATEGIAS DE LIMPIEZA:")
print("   Ubicaciones nulas: Eliminar (cr√≠tico para joins)")
print("   Passenger_count nulos: Rellenar con valor t√≠pico (1)")
print("   Tarifas nulas: Revisar caso por caso")
```

```python
# === PRIMER JOIN: TRIPS + ZONES ===

# 1. Hacer join de trips con zones para obtener informaci√≥n geogr√°fica
print("Realizando join: trips + zones...")
trips_with_zones = pd.merge(trips, zones,   # m√©todo principal para unir DataFrames
                                left_on="dolocationid",   # columna de trips que contiene ID de zona de pickup
                                right_on="locationid",  # columna de zones que contiene ID correspondiente
                                how="inner")       # tipo de join que mantiene todos los trips

print(f"   Registros antes del join: {len(trips)}")
print(f"   Registros despu√©s del join: {len(trips_with_zones)}")
print(f"   Nuevas columnas a√±adidas: {[col for col in trips_with_zones.columns if col not in trips.columns]}")

# 2. Verificar el resultado del join
print("\nVERIFICACI√ìN DEL JOIN:")
print("Conteo por Borough:")
print(trips_with_zones["borough"].value_counts())

# 3. Verificar si hay valores nulos despu√©s del join
null_after_join = trips_with_zones["borough"].isna().sum()  # contar nulos en columna borough
print(f"\nViajes sin borough asignado: {null_after_join}")

if null_after_join > 0:
    print("   Algunos viajes no encontraron su zona correspondiente")
    print("   LocationIDs problem√°ticos:")
    problematic_ids = trips_with_zones[trips_with_zones["borough"].isna()]["pulocationid"].unique()  # filtrar filas con nulos
    print(f"   {problematic_ids}")

# 4. Mostrar muestra del resultado
print("\nMUESTRA DEL DATASET INTEGRADO:")
print(trips_with_zones[["pulocationid", "borough", "zone", "trip_distance", "total_amount"]].head())
```

```python
# === SEGUNDO JOIN: TRIPS_ZONES + CALENDAR ===

# 1. Hacer join con datos de calendario
print("Realizando join: trips_zones + calendar...")
trips_complete = trips_with_zones.merge(calendar,   # mismo m√©todo de join que antes
                                         left_on="pickup_date",   # columna de fecha que creamos en trips
                                         right_on="date",  # columna de fecha en calendar
                                         how="left")       # tipo que mantiene todos los trips aunque no haya evento especial

print(f"   Registros antes del join: {len(trips_with_zones)}")
print(f"   Registros despu√©s del join: {len(trips_complete)}")

# 2. Crear flag de evento especial
trips_complete['is_special_day'] = trips_complete['special'].fillna('False')  # m√©todo para rellenar nulos con valor por defecto

print("\nDISTRIBUCI√ìN DE D√çAS ESPECIALES:")
print(trips_complete['is_special_day'].value_counts())
print("\nEjemplos de eventos especiales:")
special_days = trips_complete[trips_complete['is_special_day'] == True]
if len(special_days) > 0:
    print(special_days[['pickup_date', 'special', 'borough']].drop_duplicates())
else:
    print("   No hay eventos especiales en este per√≠odo")

# 3. Mostrar dataset final integrado
print("\nDATASET FINAL INTEGRADO:")
print(f"   Total registros: {len(trips_complete)}")
print(f"   Total columnas: {len(trips_complete.columns)}")
print(f"   Columnas principales: {['borough', 'zone', 'is_special_day', 'trip_distance', 'total_amount']}")

# 4. Verificar integridad de los datos finales
print("\nVERIFICACI√ìN FINAL:")
print("Datos faltantes por columna clave:")
key_columns = ['borough', 'zone', 'trip_distance', 'total_amount', 'is_special_day']
for col in key_columns:
    missing = trips_complete[col].isna().sum()  # verificar nulos en cada columna clave final
    print(f"   {col}: {missing} nulos")
```

```python
# === AN√ÅLISIS AGREGADO POR BOROUGH ===

# 1. An√°lisis b√°sico por borough (con dataset grande)
print("An√°lisis por Borough (procesando datos grandes)...")
borough_analysis = trips_complete.groupby(by="borough").agg({   # m√©todo para agrupar datos, por qu√© columna geogr√°fica?
    'pulocationid': "count",  # funci√≥n para contar n√∫mero de registros/viajes
    'trip_distance': ["mean", "std", "median"],  # funci√≥n para promedio + desviaci√≥n + mediana
    'total_amount': ["mean", "std", "median"],   # mismas estad√≠sticas para tarifas
    'fare_amount': "mean",     # solo promedio de tarifa base
    'tip_amount': ['mean', 'median'],  # estad√≠sticas de propinas
    'passenger_count': "mean"  # funci√≥n para promedio de pasajeros
}).round(2)

# Aplanar columnas multi-nivel
borough_analysis.columns = ['num_trips', 'avg_distance', 'std_distance', 'median_distance',
                           'avg_total', 'std_total', 'median_total', 'avg_fare',
                           'avg_tip', 'median_tip', 'avg_passengers']

# Ordenar por n√∫mero de viajes
borough_analysis = borough_analysis.sort_values(by='num_trips', ascending=False)  # m√©todo para ordenar DataFrame por una columna espec√≠fica

print("\nAN√ÅLISIS COMPLETO POR BOROUGH:")
print(borough_analysis)

# 2. Calcular m√©tricas adicionales empresariales
borough_analysis['revenue_per_km'] = (borough_analysis['avg_total'] /
                                     borough_analysis['avg_distance']).round(2)
borough_analysis['tip_rate'] = (borough_analysis['avg_tip'] /
                               borough_analysis['avg_fare'] * 100).round(1)
borough_analysis['market_share'] = (borough_analysis['num_trips'] /
                                  borough_analysis['num_trips'].sum() * 100).round(1)

print("\nAN√ÅLISIS CON M√âTRICAS EMPRESARIALES:")
print(borough_analysis[['num_trips', 'market_share', 'revenue_per_km', 'tip_rate']])

# 3. Encontrar insights
print("\nINSIGHTS PRINCIPALES:")
print(f"   Borough con m√°s viajes: {borough_analysis.index[0]}")
print(f"   Borough con viajes m√°s largos: {borough_analysis['avg_distance'].idxmax()}")
print(f"   Borough con tarifas m√°s altas: {borough_analysis['avg_total'].idxmax()}")
print(f"   Mejor revenue por km: {borough_analysis['revenue_per_km'].idxmax()}")
```

```python
# === AN√ÅLISIS COMPARATIVO: D√çAS NORMALES VS ESPECIALES ===

# 1. An√°lisis por borough y tipo de d√≠a
print("üìÖ An√°lisis: Borough + D√≠a Especial...")
borough_day_analysis = trips_complete.groupby(by=["borough", "is_special_day"]).agg({  # agrupar por DOS columnas: geograf√≠a y tipo de d√≠a
    'pulocationid': "count",  # funci√≥n para contar viajes
    'trip_distance': "mean",  # funci√≥n para promedio de distancia
    'total_amount': "mean"    # funci√≥n para promedio de tarifa
}).round(2)

borough_day_analysis.columns = ['num_trips', 'avg_distance', 'avg_total']

print("\nüìä AN√ÅLISIS BOROUGH + D√çA ESPECIAL:")
print(borough_day_analysis)

# 2. Comparar d√≠as normales vs especiales
print("\nüîç COMPARACI√ìN D√çAS NORMALES VS ESPECIALES:")

# Pivotear para comparar f√°cilmente
comparison = trips_complete.groupby(by='is_special_day').agg({  # agrupar solo por tipo de d√≠a para comparaci√≥n general
    'trip_distance': 'mean',    # promedio de distancia por tipo de d√≠a
    'total_amount': 'mean',     # promedio de tarifa por tipo de d√≠a
    'pulocationid': 'count'     # conteo de viajes por tipo de d√≠a
}).round(2)

# Renombrar √≠ndices seg√∫n los valores √∫nicos encontrados
unique_day_types = comparison.index.tolist()
if len(unique_day_types) == 2:
    comparison.index = ['D√≠a Normal', 'D√≠a Especial']
elif len(unique_day_types) == 1:
    if unique_day_types[0] in ['False', False]:
        comparison.index = ['D√≠a Normal']
    else:
        comparison.index = ['D√≠a Especial']

comparison.columns = ['Avg Distance', 'Avg Amount', 'Num Trips']

print(comparison)

# 3. Calcular diferencias porcentuales
if len(comparison) > 1:
    # Hay tanto d√≠as normales como especiales
    if 'D√≠a Normal' in comparison.index and 'D√≠a Especial' in comparison.index:
        normal_day = comparison.loc['D√≠a Normal']
        special_day = comparison.loc['D√≠a Especial']

        print("\nIMPACTO DE D√çAS ESPECIALES:")
        distance_change = ((special_day['Avg Distance'] - normal_day['Avg Distance']) / normal_day['Avg Distance'] * 100)
        amount_change = ((special_day['Avg Amount'] - normal_day['Avg Amount']) / normal_day['Avg Amount'] * 100)

        print(f"   Cambio en distancia promedio: {distance_change:+.1f}%")
        print(f"   Cambio en tarifa promedio: {amount_change:+.1f}%")
    else:
        print("\nINFORMACI√ìN DE D√çAS:")
        for idx, row in comparison.iterrows():
            print(f"   {idx}: {row['Num Trips']:,} viajes, ${row['Avg Amount']:.2f} promedio")
else:
    print(f"\nSOLO HAY {comparison.index[0]}:")
    print(f"   Viajes: {comparison.iloc[0]['Num Trips']:,}")
    print(f"   Distancia promedio: {comparison.iloc[0]['Avg Distance']:.2f} millas")
    print(f"   Tarifa promedio: ${comparison.iloc[0]['Avg Amount']:.2f}")
    print("   No hay datos de d√≠as especiales para comparar en este per√≠odo")
```


```python
# === T√âCNICAS PARA TRABAJAR CON DATASETS GRANDES ===

# 1. Sampling estrat√©gico para visualizaciones
print("‚ö° Aplicando t√©cnicas para datasets grandes...")

# Si el dataset es muy grande, usar muestra para visualizaciones
if len(trips_complete) > 50000:
    print(f"   üìä Dataset grande detectado: {len(trips_complete):,} registros")
    print("   üéØ Creando muestra estratificada para visualizaciones...")

    # Muestra proporcional por borough
    sample_size = min(10000, len(trips_complete) // 10)
    trips_sample = trips_complete.sample(n=sample_size, random_state=42)  # m√©todo para tomar muestra aleatoria de n registros

    print(f"   ‚úÖ Muestra creada: {len(trips_sample):,} registros ({len(trips_sample)/len(trips_complete)*100:.1f}%)")
else:
    trips_sample = trips_complete
    print("   ‚ÑπÔ∏è Dataset peque√±o, usando datos completos para visualizaci√≥n")

# 2. An√°lisis de performance de joins
print("\nüìà AN√ÅLISIS DE PERFORMANCE:")
join_stats = {
    'total_trips': len(trips),
    'matched_zones': (trips_complete['borough'].notna()).sum(),
    'match_rate': (trips_complete['borough'].notna().sum() / len(trips) * 100),
    'unique_zones_used': trips_complete['zone'].nunique(),
    'total_zones_available': len(zones),
    'zone_coverage': (trips_complete['zone'].nunique() / len(zones) * 100)
}

for key, value in join_stats.items():
    if 'rate' in key or 'coverage' in key:
        print(f"   {key}: {value:.1f}%")
    else:
        print(f"   {key}: {value:,}")

# 3. An√°lisis temporal avanzado (solo si hay suficientes datos)
if len(trips_complete) > 1000:
    print("\nüìÖ AN√ÅLISIS TEMPORAL AVANZADO:")

    # An√°lisis por hora del d√≠a
    trips_complete['pickup_hour'] = trips_complete['tpep_pickup_datetime'].dt.hour  # extraer hora de la fecha/hora
    hourly_analysis = trips_complete.groupby(by='pickup_hour').agg({  # agrupar por hora del d√≠a
        'pulocationid': 'count',     # contar viajes por hora
        'total_amount': 'mean',      # tarifa promedio por hora
        'trip_distance': 'mean'      # distancia promedio por hora
    }).round(2)

    hourly_analysis.columns = ['trips_count', 'avg_amount', 'avg_distance']

    print("   ‚è∞ Horas pico por n√∫mero de viajes:")
    peak_hours = hourly_analysis.sort_values(by='trips_count', ascending=False).head(3)  # ordenar por m√°s viajes, tomar top 3
    for hour, stats in peak_hours.iterrows():
        print(f"      {hour:02d}:00 - {stats['trips_count']:,} viajes")
```

```python
# === AN√ÅLISIS DE CORRELACIONES NUM√âRICAS ===

# Calcular correlaciones entre variables num√©ricas
print("Calculando correlaciones entre variables num√©ricas...")
numeric_cols = ['trip_distance', 'total_amount', 'fare_amount', 'tip_amount']
corr_matrix = trips_complete[numeric_cols].corr()  # m√©todo para calcular matriz de correlaci√≥n

print("\nMatriz de Correlaci√≥n:")
print(corr_matrix.round(3))

print("\nCorrelaciones m√°s fuertes:")
corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i+1, len(corr_matrix.columns)):
        corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))

corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)
for var1, var2, corr in corr_pairs[:3]:
    print(f"   {var1} vs {var2}: {corr:.3f}")

print("\nINTERPRETACI√ìN DE CORRELACIONES:")
print("   > 0.7: Correlaci√≥n fuerte positiva")
print("   0.3-0.7: Correlaci√≥n moderada positiva")
print("   -0.3-0.3: Correlaci√≥n d√©bil")
print("   < -0.7: Correlaci√≥n fuerte negativa")
```

###**1. ¬øQu√© diferencia hay entre un LEFT JOIN y un INNER JOIN?**

INNER JOIN: conserva solo las filas donde existe coincidencia en ambas tablas.

LEFT JOIN: conserva todas las filas de la tabla izquierda, aunque no haya coincidencia en la tabla derecha. Si no encuentra una coincidencia, las columnas de la derecha quedan como NULL.


###**2. ¬øPor qu√© usamos LEFT JOIN en lugar de INNER JOIN para trips+zones?**

Porque queremos mantener todos los viajes, incluso si alguno no tiene zona asignada en la tabla de zonas.


###**3. ¬øQu√© problemas pueden surgir al hacer joins con datos de fechas?**

Puede surgir que hayan datos en formatos distintos (string) y (datetime), como tambi√©n una fecha completa (con hora) o solo la fecha.
Pueden haber distintas zonas horarias, para esto se debe de normalizar.

###**4. ¬øCu√°l es la ventaja de integrar m√∫ltiples fuentes de datos?**


Permite tener un contexto m√°s completo: no solo ver los viajes, sino tambi√©n las zonas y los eventos.

El cruzar datos permite visualizar nuevas vistas y observar como estos influyen entre s√≠. Por ejemplo: cruzar viajes, zonas y calendario.

###**5. ¬øQu√© insights de negocio obtuviste del an√°lisis integrado?**

**Patrones por Zona:**

*   Manhattan concentra la mayor cantidad de viajes, tarifas m√°s altas y mayor revenue por km.
*   Staten Island y Bronx tienen baja actividad, demuestra una brecha en la cobertura comparando con el resto.
*   Las propinas son frecuentes en Manhattan.

**Impacto en Calendario:**

*   En d√≠as con eventos aumentan los viajes totales, la tarifa promedio y la distancia recorrida.

**Oportunidades:**

*   Planificar flota y turnos reforzando disponibilidad en eventos y horarios pico.
*   Expansi√≥n en boroughs con baja cobertura (como lo son Staten Island y Bronx)
*   Se podr√≠an mejorar tarifas y servicios en zonas de alto turismo y negocios (Manhattan) para ayudar a mantener o mejorar revenue o tarifas.

```python
# === SETUP PREFECT ===

# Instalar Prefect (si no est√° instalado)
!pip install prefect

import prefect
from prefect import task, flow, get_run_logger
import pandas as pd

print("Prefect instalado y configurado")
print(f"   Versi√≥n: {prefect.__version__}")
```

```python
# === TASKS SIMPLES PARA APRENDER PREFECT ===

@task(name="Cargar Datos", retries=2, retry_delay_seconds=3)
def cargar_datos(url: str, tipo: str) -> pd.DataFrame:
    """Task simple para cargar cualquier tipo de datos"""
    logger = get_run_logger()
    logger.info(f"Cargando {tipo} desde: {url}")

    # Cargar seg√∫n el tipo
    if tipo == "trips":
        data = pd.read_parquet(url)  # funci√≥n para Parquet
    elif tipo == "zones":
        data = pd.read_csv(url)  # funci√≥n para CSV
    else:  # calendar
        data = pd.read_json(url)  # funci√≥n para JSON
        data['date'] = pd.to_datetime(data['date']).dt.date  # convertir a fechas

    logger.info(f"{tipo} cargado: {data.shape[0]} filas")
    return data

@task(name="Hacer Join Simple")
def hacer_join_simple(trips: pd.DataFrame, zones: pd.DataFrame) -> pd.DataFrame:
    """Task para hacer join b√°sico de trips + zones"""
    logger = get_run_logger()
    logger.info("Haciendo join simple...")

    # Normalizar columnas
    trips.columns = trips.columns.str.lower()  # convertir a min√∫sculas
    zones.columns = zones.columns.str.lower()  # misma transformaci√≥n

    # Join b√°sico
    resultado = trips.merge(zones,   # m√©todo para unir DataFrames
                             left_on="pulocationid",   # columna de pickup location en trips
                             right_on="locationid",  # columna de location en zones
                             how="left")       # tipo de join que mantiene todos los trips

    logger.info(f"Join completado: {len(resultado)} registros")
    return resultado

@task(name="An√°lisis R√°pido")
def analisis_rapido(data: pd.DataFrame) -> dict:
    """Task para an√°lisis b√°sico"""
    logger = get_run_logger()
    logger.info("Haciendo an√°lisis b√°sico...")

    # Stats simples
    stats = {
        'total_registros': len(data),
        'boroughs': data['borough'].value_counts().head(3).to_dict(),  # m√©todo para contar valores
        'distancia_promedio': round(data['trip_distance'].mean(), 2),  # m√©todo para promedio
        'tarifa_promedio': round(data['total_amount'].mean(), 2)  # m√©todo para promedio
    }

    logger.info(f"An√°lisis completado: {stats['total_registros']} registros")
    return stats
```

```python
# === FLOW PRINCIPAL (EL PIPELINE COMPLETO) ===

@flow(name="Pipeline Simple NYC Taxi")
def pipeline_taxi_simple():
    """
    Flow simple que conecta todos los tasks
    """
    logger = get_run_logger()
    logger.info("Iniciando pipeline simple...")

    # URLs de datos
    trips_url = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet"
    zones_url = "https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv"

    # PASO 1: Cargar datos (con retry autom√°tico si falla)
    logger.info("Paso 1: Cargando datos...")
    trips = cargar_datos(trips_url, "trips")  # tipo de datos trips
    zones = cargar_datos(zones_url, "zones")  # tipo de datos zones

    # PASO 2: Hacer join
    logger.info("Paso 2: Haciendo join...")
    data_unida = hacer_join_simple(trips, zones)

    # PASO 3: An√°lisis b√°sico
    logger.info("Paso 3: Analizando...")
    resultados = analisis_rapido(data_unida)

    # PASO 4: Mostrar resultados
    logger.info("Pipeline completado!")
    logger.info(f"Resultados: {resultados}")

    return resultados
```

```python
# === EJECUTAR EL PIPELINE ===

if __name__ == "__main__":
    print("Ejecutando pipeline simple...")

    # Ejecutar el flow
    resultado = pipeline_taxi_simple()  # nombre de la funci√≥n del flow

    print("\nRESULTADOS FINALES:")
    print(f"   Total registros: {resultado['total_registros']:,}")
    print(f"   Distancia promedio: {resultado['distancia_promedio']} millas")
    print(f"   Tarifa promedio: ${resultado['tarifa_promedio']}")
    print("\nTop 3 Boroughs:")
    for borough, count in resultado["boroughs"].items():  # clave del diccionario que contiene boroughs
        print(f"   {borough}: {count:,} viajes")
```

```python
!prefect server start
```

**
¬øQu√© ventaja tiene usar @task en lugar de una funci√≥n normal? PISTA: ¬øQu√© pasa si la carga de datos falla temporalmente?
- Permite reintentos autom√°ticos, manejo de errores y monitoreo. Si la carga falla, Prefect puede reintentar el task seg√∫n lo configurado.

¬øPara qu√© sirve el @flow decorator? PISTA: ¬øC√≥mo conecta y organiza los tasks?
- Define y organiza el pipeline, conectando tasks en un flujo l√≥gico y permitiendo su ejecuci√≥n secuencial o paralela.

¬øEn qu√© casos reales usar√≠as esto? PISTA: Reportes diarios, an√°lisis autom√°ticos, pipelines de ML
- Automatizaci√≥n de reportes, an√°lisis recurrentes, ETL, procesamiento de datos, y entrenamiento de modelos de machine learning.