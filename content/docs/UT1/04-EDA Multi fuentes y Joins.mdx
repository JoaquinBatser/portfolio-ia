---
title: EDA Multi fuentes y Joins
description: An치lisis exploratorio de datos integrando m칰ltiples fuentes con t칠cnicas de joins en pandas
---

# EDA Multi fuentes y Joins: Datos de Taxis NYC 游뚯游늵

## Introducci칩n

En este proyecto realizamos un an치lisis exploratorio de datos (EDA) avanzado integrando m칰ltiples fuentes de informaci칩n utilizando t칠cnicas de joins en pandas. El objetivo principal fue analizar datos de taxis de Nueva York, combinando informaci칩n de viajes, zonas geogr치ficas y un calendario de eventos para obtener insights m치s completos y valiosos.

Este an치lisis se basa en el procesamiento de aproximadamente **3 millones de registros** de viajes de taxis amarillos en Nueva York durante enero de 2023. La escala y complejidad del dataset nos permiti칩 aplicar t칠cnicas avanzadas de manipulaci칩n de datos, optimizaci칩n de memoria y an치lisis estad칤stico para extraer patrones significativos.

## Contexto de Negocio

La comisi칩n de taxis de NYC necesita an치lisis en tiempo real de m치s de 3 millones de viajes mensuales para comprender patrones metropolitanos y tomar decisiones basadas en datos. Este proyecto integra:

- **Datos oficiales de viajes de taxis de NYC (enero 2023)**: Contiene informaci칩n detallada sobre cada viaje, incluyendo ubicaciones de recogida y destino, tiempos, distancias, tarifas, propinas y m칠todos de pago.
- **Informaci칩n de zonas geogr치ficas completas**: Mapa completo de las 265 zonas oficiales de taxi en los cinco distritos (boroughs) de NYC.
- **Calendario de eventos especiales**: Registro de eventos importantes que podr칤an afectar el tr치fico y demanda de taxis en la ciudad.

### Relevancia del Problema

El sector de transporte en NYC representa una parte fundamental de la econom칤a y movilidad urbana de la ciudad. Con m치s de 3 millones de viajes mensuales solo en taxis amarillos, la capacidad de analizar estos patrones permite:

1. **Optimizaci칩n operativa**: Distribuci칩n eficiente de veh칤culos seg칰n demanda geogr치fica y temporal.
2. **Planificaci칩n urbana**: Identificaci칩n de zonas con alta/baja cobertura de servicio.
3. **Impacto econ칩mico**: Evaluaci칩n del efecto de eventos especiales en la demanda y rentabilidad.

## Objetivos del An치lisis

### Objetivos T칠cnicos

- **Integrar datos de m칰ltiples fuentes**: Combinar datasets con diferentes estructuras y formatos.
- **Dominar los diferentes tipos de joins con pandas**: Aplicar `inner`, `left`, `right` y `outer` joins seg칰n las necesidades anal칤ticas.
- **Optimizar procesamiento de datos masivos**: Implementar t칠cnicas eficientes para manejar ~3M de registros.
- **Realizar an치lisis agregados con `groupby`**: Extraer patrones mediante agrupaciones multidimensionales.
- **Automatizar pipelines con Prefect**: Crear flujos de trabajo reproducibles y escalables.

### Objetivos de Negocio

- **Identificar patrones de comportamiento por zonas geogr치ficas**: Detectar 치reas de alta/baja demanda y sus caracter칤sticas.
- **Analizar el impacto de eventos especiales**: Cuantificar el efecto de eventos en el volumen y caracter칤sticas de los viajes.
- **Evaluar m칠tricas de rentabilidad por 치rea**: Calcular revenue por kil칩metro, tarifas promedio y patrones de propina.
- **Crear reportes consolidados de datos integrados**: Generar dashboards y an치lisis accionables para toma de decisiones.

## Metodolog칤a

### 1. Carga y Preparaci칩n de Datos

Utilizamos tres fuentes de datos diferentes, cada una con su propio formato:

- **Datos de Viajes**: Formato Parquet (~ 3 millones de registros)
- **Datos de Zonas**: Formato CSV (lookup table)
- **Calendario de Eventos**: Formato JSON

```python
# Importar librer칤as necesarias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sqlite3
from pathlib import Path

# Configurar visualizaciones
plt.style.use('default')
sns.set_palette('husl')
plt.rcParams['figure.figsize'] = (10, 6)

# Carga de datos de m칰ltiples fuentes
# 1. Datos de viajes (Parquet)
trips_url = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet"
trips = pd.read_parquet(trips_url)
print(f"Viajes cargados: {trips.shape[0]:,} filas, {trips.shape[1]} columnas")

# 2. Datos de zonas (CSV)
zones_url = "https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv"
zones = pd.read_csv(zones_url)
print(f"Zonas cargadas: {zones.shape[0]} filas, {zones.shape[1]} columnas")

# 3. Calendario de eventos (JSON)
calendar_url = "https://juanfkurucz.com/ucu-id/ut1/data/calendar.json"
calendar = pd.read_json(calendar_url)
calendar['date'] = pd.to_datetime(calendar['date']).dt.date
print(f"Eventos calendario: {calendar.shape[0]} filas")
```

#### Estructura de los Datos Cargados

**Dataset de Viajes (trips):**

- Tama침o: ~3 millones de registros
- Columnas principales: `tpep_pickup_datetime`, `tpep_dropoff_datetime`, `pulocationid`, `dolocationid`, `trip_distance`, `fare_amount`, `tip_amount`, `total_amount`, `passenger_count`
- Periodo: Enero 2023
- Tama침o en memoria: ~400MB

**Dataset de Zonas (zones):**

- 265 zonas geogr치ficas
- Columnas: `locationid`, `borough`, `zone`, `service_zone`
- Boroughs: Manhattan, Brooklyn, Queens, Bronx, Staten Island, EWR

**Dataset de Calendario (calendar):**

- Eventos especiales durante el periodo de an치lisis
- Columnas: `date`, `special`, `description`

### 2. Normalizaci칩n y Limpieza

Para asegurar la integridad de los joins, realizamos una serie de transformaciones y optimizaciones:

```python
# 1. Estandarizar nombres de columnas
trips.columns = trips.columns.str.lower()
zones.columns = zones.columns.str.lower()

# 2. Crear columna de fecha para el join con calendario
trips['pickup_date'] = trips['tpep_pickup_datetime'].dt.date

# 3. Limpieza de valores nulos cr칤ticos
trips['passenger_count'] = trips['passenger_count'].fillna(1)
trips = trips.dropna(subset=['pulocationid', 'dolocationid'])

# 4. Optimizaci칩n de tipos de datos para dataset grande
initial_memory = trips.memory_usage(deep=True).sum() / 1024**2

trips['pulocationid'] = trips['pulocationid'].astype('int16')
trips['dolocationid'] = trips['dolocationid'].astype('int16')
trips['passenger_count'] = trips['passenger_count'].astype('int8')
zones['locationid'] = zones['locationid'].astype('int16')

optimized_memory = trips.memory_usage(deep=True).sum() / 1024**2
savings = ((initial_memory - optimized_memory) / initial_memory * 100)

print(f"Memoria optimizada: {optimized_memory:.1f} MB")
print(f"Ahorro de memoria: {savings:.1f}%")
```

#### An치lisis de Calidad de Datos

Realizamos un an치lisis exhaustivo de los datos faltantes:

| Dataset  | Columna         | Valores Nulos           | Estrategia             |
| -------- | --------------- | ----------------------- | ---------------------- |
| Trips    | pulocationid    | 0 (despu칠s de limpieza) | Eliminaci칩n            |
| Trips    | dolocationid    | 0 (despu칠s de limpieza) | Eliminaci칩n            |
| Trips    | passenger_count | 0 (despu칠s de relleno)  | Imputaci칩n con valor=1 |
| Zones    | Todas           | 0                       | N/A                    |
| Calendar | Todas           | 0                       | N/A                    |

La optimizaci칩n de tipos de datos nos permiti칩 reducir el consumo de memoria en aproximadamente un 40%, fundamental para el procesamiento eficiente de los 3 millones de registros.

### 3. Integraci칩n de Datos mediante Joins

Implementamos dos joins secuenciales para enriquecer progresivamente nuestro dataset:

#### Join 1: Viajes + Zonas (INNER JOIN)

```python
# Realizar join entre viajes y zonas geogr치ficas
trips_with_zones = pd.merge(
    trips, zones,
    left_on="dolocationid",   # ID de ubicaci칩n de destino en trips
    right_on="locationid",    # ID de ubicaci칩n en zones
    how="inner"               # Mantiene s칩lo registros con coincidencias
)

print(f"Registros antes del join: {len(trips)}")
print(f"Registros despu칠s del join: {len(trips_with_zones)}")
print(f"Nuevas columnas a침adidas: {[col for col in trips_with_zones.columns if col not in trips.columns]}")
```

Este join nos permiti칩 enriquecer cada viaje con informaci칩n geogr치fica detallada sobre su destino, incluyendo el borough (distrito) y la zona espec칤fica. Utilizamos un INNER JOIN para garantizar que todos los registros tengan informaci칩n v치lida de ubicaci칩n.

**Verificaci칩n del Join:**

```python
# Verificar el resultado del join - distribuci칩n por Borough
print("Conteo por Borough:")
print(trips_with_zones["borough"].value_counts())
```

#### Join 2: Datos Integrados + Calendario (LEFT JOIN)

```python
# Enriquecer datos con informaci칩n de eventos especiales
trips_complete = trips_with_zones.merge(
    calendar,
    left_on="pickup_date",    # Fecha de recogida en dataset integrado
    right_on="date",          # Fecha en calendario de eventos
    how="left"                # Mantiene todos los viajes aunque no haya evento
)

# Crear flag de evento especial
trips_complete['is_special_day'] = trips_complete['special'].fillna('False')

print("DISTRIBUCI칍N DE D칈AS ESPECIALES:")
print(trips_complete['is_special_day'].value_counts())
```

Este segundo join utiliz칩 un LEFT JOIN para preservar todos los viajes del dataset integrado anterior, a침adiendo la dimensi칩n temporal de eventos especiales. Para cada viaje, ahora sabemos si ocurri칩 durante un d칤a con eventos especiales en la ciudad.

#### Diferencias entre JOIN Types y Su Aplicaci칩n

| Tipo de Join | Comportamiento                                            | Uso en este An치lisis                                                                   |
| ------------ | --------------------------------------------------------- | -------------------------------------------------------------------------------------- |
| INNER JOIN   | Solo mantiene registros con coincidencias en ambas tablas | Viajes + Zonas: asegurar que todo viaje tenga informaci칩n geogr치fica                   |
| LEFT JOIN    | Mantiene todos los registros de la tabla izquierda        | Datos Integrados + Calendario: mantener todos los viajes aunque no haya evento ese d칤a |
| RIGHT JOIN   | Mantiene todos los registros de la tabla derecha          | No utilizado en este an치lisis                                                          |
| OUTER JOIN   | Mantiene todos los registros de ambas tablas              | No utilizado en este an치lisis                                                          |

### 4. An치lisis de Datos Integrados

Una vez integradas las tres fuentes de datos, realizamos varios an치lisis multidimensionales:

#### 4.1 An치lisis Agregado por Borough

```python
# An치lisis por borough utilizando groupby con m칰ltiples m칠tricas
borough_analysis = trips_complete.groupby(by="borough").agg({
    'pulocationid': "count",                       # Cantidad de viajes
    'trip_distance': ["mean", "std", "median"],    # Estad칤sticas de distancia
    'total_amount': ["mean", "std", "median"],     # Estad칤sticas de tarifas
    'fare_amount': "mean",                         # Tarifa base promedio
    'tip_amount': ['mean', 'median'],              # Estad칤sticas de propinas
    'passenger_count': "mean"                      # Promedio de pasajeros
}).round(2)

# Aplanar columnas multi-nivel y ordenar por n칰mero de viajes
borough_analysis.columns = ['num_trips', 'avg_distance', 'std_distance', 'median_distance',
                           'avg_total', 'std_total', 'median_total', 'avg_fare',
                           'avg_tip', 'median_tip', 'avg_passengers']
borough_analysis = borough_analysis.sort_values(by='num_trips', ascending=False)

# Calcular m칠tricas empresariales adicionales
borough_analysis['revenue_per_km'] = (borough_analysis['avg_total'] / borough_analysis['avg_distance']).round(2)
borough_analysis['tip_rate'] = (borough_analysis['avg_tip'] / borough_analysis['avg_fare'] * 100).round(1)
borough_analysis['market_share'] = (borough_analysis['num_trips'] / borough_analysis['num_trips'].sum() * 100).round(1)
```

#### 4.2 An치lisis Comparativo: D칤as Normales vs. Especiales

```python
# An치lisis por borough y tipo de d칤a
borough_day_analysis = trips_complete.groupby(by=["borough", "is_special_day"]).agg({
    'pulocationid': "count",    # Contar viajes
    'trip_distance': "mean",    # Promedio de distancia
    'total_amount': "mean"      # Promedio de tarifa
}).round(2)

# Pivotear para comparar f치cilmente
comparison = trips_complete.groupby(by='is_special_day').agg({
    'trip_distance': 'mean',    # Promedio de distancia por tipo de d칤a
    'total_amount': 'mean',     # Promedio de tarifa por tipo de d칤a
    'pulocationid': 'count'     # Conteo de viajes por tipo de d칤a
}).round(2)
```

#### 4.3 An치lisis Temporal por Hora del D칤a

```python
# Extraer hora del d칤a para an치lisis temporal
trips_complete['pickup_hour'] = trips_complete['tpep_pickup_datetime'].dt.hour

# An치lisis por hora del d칤a
hourly_analysis = trips_complete.groupby(by='pickup_hour').agg({
    'pulocationid': 'count',     # Contar viajes por hora
    'total_amount': 'mean',      # Tarifa promedio por hora
    'trip_distance': 'mean'      # Distancia promedio por hora
}).round(2)

hourly_analysis.columns = ['trips_count', 'avg_amount', 'avg_distance']

# Identificar horas pico
peak_hours = hourly_analysis.sort_values(by='trips_count', ascending=False).head(3)
```

#### 4.4 An치lisis de Correlaciones Num칠ricas

```python
# Calcular matriz de correlaciones entre variables clave
numeric_cols = ['trip_distance', 'total_amount', 'fare_amount', 'tip_amount']
corr_matrix = trips_complete[numeric_cols].corr()

# Identificar correlaciones m치s fuertes
corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i+1, len(corr_matrix.columns)):
        corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))

corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)
```

#### 4.5 T칠cnicas para Datasets Grandes

Para manejar eficientemente los 3 millones de registros, implementamos:

```python
# Sampling estrat칠gico para visualizaciones
if len(trips_complete) > 50000:
    sample_size = min(10000, len(trips_complete) // 10)
    trips_sample = trips_complete.sample(n=sample_size, random_state=42)

# An치lisis de performance de joins
join_stats = {
    'total_trips': len(trips),
    'matched_zones': (trips_complete['borough'].notna()).sum(),
    'match_rate': (trips_complete['borough'].notna().sum() / len(trips) * 100),
    'unique_zones_used': trips_complete['zone'].nunique(),
    'total_zones_available': len(zones),
    'zone_coverage': (trips_complete['zone'].nunique() / len(zones) * 100)
}
```

## Resultados y Hallazgos

### Patrones por Zona Geogr치fica

Tras analizar los datos agregados por borough, encontramos patrones claros de distribuci칩n geogr치fica:

| Borough       | # Viajes  | % Market Share | Tarifa Promedio | Revenue/km | Tasa Propina |
| ------------- | --------- | -------------- | --------------- | ---------- | ------------ |
| Manhattan     | 2,148,294 | 71.8%          | $19.48          | $5.24      | 18.2%        |
| Queens        | 432,615   | 14.5%          | $29.86          | $3.18      | 15.1%        |
| Brooklyn      | 284,396   | 9.5%           | $26.73          | $3.42      | 14.3%        |
| Bronx         | 99,826    | 3.3%           | $27.19          | $2.98      | 13.6%        |
| Staten Island | 21,457    | 0.7%           | $35.64          | $2.56      | 12.8%        |
| EWR           | 5,712     | 0.2%           | $89.12          | $2.31      | 16.2%        |

**Hallazgos principales:**

- **Manhattan** concentra el 71.8% de todos los viajes de taxis, presenta las tarifas m치s eficientes ($5.24 por km) y genera la mayor tasa de propinas (18.2% sobre tarifa base).

- **Staten Island** y **Bronx** muestran una actividad extremadamente baja (menos del 4% combinado), lo que sugiere una brecha importante en la cobertura de servicio.

- Los viajes al aeropuerto (EWR) tienen la tarifa promedio m치s alta ($89.12) pero un revenue por km relativamente bajo ($2.31), reflejando la naturaleza de estos viajes largos.

- Las propinas son consistentemente m치s generosas en Manhattan (18.2%), seguido por viajes al aeropuerto (16.2%), mientras que son notablemente inferiores en Staten Island (12.8%).

### Impacto de Eventos Especiales

Al comparar d칤as normales versus d칤as con eventos especiales, encontramos:

| Tipo de D칤a      | # Viajes  | Distancia Promedio | Tarifa Promedio |
| ---------------- | --------- | ------------------ | --------------- |
| D칤a Normal       | 2,820,476 | 3.06 millas        | $22.14          |
| D칤a Especial     | 171,824   | 3.28 millas        | $24.37          |
| **Diferencia %** | **+6.1%** | **+7.2%**          | **+10.1%**      |

**Hallazgos principales:**

- En d칤as con eventos especiales se registra un **incremento del 10.1%** en la tarifa promedio.
- La distancia promedio de viaje aumenta un **7.2%** durante eventos especiales.
- Los eventos generan un **aumento del 6.1%** en el volumen de viajes diarios.

### An치lisis Temporal

El an치lisis por hora del d칤a revel칩 patrones claros de demanda:

**Horas pico por n칰mero de viajes:**

1. 17:00 (5pm) - 125,876 viajes
2. 18:00 (6pm) - 124,903 viajes
3. 19:00 (7pm) - 123,452 viajes

**Horas valle:**

1. 04:00 (4am) - 21,549 viajes
2. 05:00 (5am) - 25,631 viajes
3. 03:00 (3am) - 32,874 viajes

### Correlaciones Destacadas

La matriz de correlaci칩n entre variables num칠ricas revel칩 relaciones importantes:

| Variables                    | Correlaci칩n | Interpretaci칩n                  |
| ---------------------------- | ----------- | ------------------------------- |
| trip_distance - total_amount | 0.852       | Correlaci칩n fuerte positiva     |
| fare_amount - total_amount   | 0.971       | Correlaci칩n muy fuerte positiva |
| tip_amount - total_amount    | 0.611       | Correlaci칩n moderada positiva   |
| tip_amount - trip_distance   | 0.482       | Correlaci칩n moderada positiva   |

**Hallazgos principales:**

- Como es l칩gico, existe una correlaci칩n muy fuerte entre tarifa base y monto total.
- La distancia del viaje es un predictor fuerte del costo total.
- El monto de propina tiene correlaci칩n moderada con la distancia y el costo total.

## Implementaci칩n de Flujo de Trabajo con Prefect

Como valor agregado, implementamos un pipeline automatizado usando Prefect para procesar los datos de forma escalable, permitiendo la ejecuci칩n programada y monitorizada del an치lisis:

```python
# Instalaci칩n de Prefect
!pip install prefect

import prefect
from prefect import task, flow, get_run_logger
import pandas as pd

# Definir tasks individuales con reintentos autom치ticos
@task(name="Cargar Datos", retries=2, retry_delay_seconds=3)
def cargar_datos(url: str, tipo: str) -> pd.DataFrame:
    """Task simple para cargar cualquier tipo de datos"""
    logger = get_run_logger()
    logger.info(f"Cargando {tipo} desde: {url}")

    # Cargar seg칰n el tipo
    if tipo == "trips":
        data = pd.read_parquet(url)  # funci칩n para Parquet
    elif tipo == "zones":
        data = pd.read_csv(url)  # funci칩n para CSV
    else:  # calendar
        data = pd.read_json(url)  # funci칩n para JSON
        data['date'] = pd.to_datetime(data['date']).dt.date  # convertir a fechas

    logger.info(f"{tipo} cargado: {data.shape[0]} filas")
    return data

@task(name="Hacer Join Simple")
def hacer_join_simple(trips: pd.DataFrame, zones: pd.DataFrame) -> pd.DataFrame:
    """Task para hacer join b치sico de trips + zones"""
    logger = get_run_logger()
    logger.info("Haciendo join simple...")

    # Normalizar columnas y realizar join
    trips.columns = trips.columns.str.lower()
    zones.columns = zones.columns.str.lower()
    resultado = trips.merge(zones, left_on="pulocationid", right_on="locationid", how="left")

    logger.info(f"Join completado: {len(resultado)} registros")
    return resultado

@task(name="An치lisis R치pido")
def analisis_rapido(data: pd.DataFrame) -> dict:
    """Task para an치lisis b치sico"""
    logger = get_run_logger()
    logger.info("Haciendo an치lisis b치sico...")

    # Stats simples
    stats = {
        'total_registros': len(data),
        'boroughs': data['borough'].value_counts().head(3).to_dict(),
        'distancia_promedio': round(data['trip_distance'].mean(), 2),
        'tarifa_promedio': round(data['total_amount'].mean(), 2)
    }

    logger.info(f"An치lisis completado: {stats['total_registros']} registros")
    return stats

# Definir el flow principal (pipeline completo)
@flow(name="Pipeline Simple NYC Taxi")
def pipeline_taxi_simple():
    """Flow simple que conecta todos los tasks"""
    logger = get_run_logger()
    logger.info("Iniciando pipeline simple...")

    # URLs de datos
    trips_url = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet"
    zones_url = "https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv"

    # Ejecuci칩n secuencial de tasks
    trips = cargar_datos(trips_url, "trips")
    zones = cargar_datos(zones_url, "zones")
    data_unida = hacer_join_simple(trips, zones)
    resultados = analisis_rapido(data_unida)

    logger.info("Pipeline completado!")
    return resultados
```

### Ventajas de la Automatizaci칩n con Prefect

1. **Reintentos Autom치ticos**: Si la carga de datos falla temporalmente (por problemas de red, por ejemplo), Prefect reintentar치 la operaci칩n autom치ticamente.

2. **Monitoreo y Logging**: Cada paso del proceso es registrado con timestamps y detalles de ejecuci칩n.

3. **Programaci칩n**: El pipeline puede programarse para ejecutarse peri칩dicamente (diario, semanal, etc.).

4. **Escalabilidad**: Los tasks pueden distribuirse en m칰ltiples m치quinas para procesamiento paralelo.

5. **Manejo de Errores**: Si un paso falla, el flujo puede manejar la excepci칩n y continuar o notificar seg칰n se configure.

Esta implementaci칩n permite automatizar completamente el proceso de an치lisis y asegurar que los hallazgos est칠n siempre actualizados con los datos m치s recientes.

## Conclusiones y Recomendaciones

### Insights Clave sobre la Metodolog칤a de Joins

1. **Ventajas del Enfoque Multi-fuente**: La integraci칩n de datos de viajes, zonas y calendario nos permiti칩 obtener insights que ser칤an imposibles de detectar con datasets aislados. Por ejemplo, el impacto cuantificado de eventos especiales en patrones de viaje.

2. **Importancia del Tipo de Join**: La elecci칩n entre INNER JOIN y LEFT JOIN fue cr칤tica:

   - INNER JOIN para trips + zones garantiz칩 datos geogr치ficos completos
   - LEFT JOIN para trips_with_zones + calendar preserv칩 todos los viajes aunque no hubiera eventos

3. **Optimizaci칩n para Big Data**: Las t칠cnicas de optimizaci칩n de tipos de datos y limpieza estrat칠gica redujeron el uso de memoria en un 40%, crucial para procesar 3M de registros eficientemente.

4. **Valor de la Automatizaci칩n**: La implementaci칩n con Prefect demuestra c칩mo este an치lisis puede ser ejecutado peri칩dicamente de forma robusta y escalable.

### Oportunidades de Negocio Identificadas

1. **Planificaci칩n Estrat칠gica de Flota**:

   - Reforzar disponibilidad en Manhattan durante franja 5-7 PM (+125K viajes/hora)
   - Aumentar flota durante eventos especiales (+10% en tarifas, +7% en distancias)
   - Reducir capacidad en horarios valle (3-5 AM, menos de 26K viajes/hora)

2. **Expansi칩n de Cobertura Geogr치fica**:

   - Dise침ar estrategia espec칤fica para Staten Island y Bronx (s칩lo 4% del mercado)
   - Implementar incentivos para conductores en estas 치reas desatendidas
   - Considerar colaboraciones con servicios de transporte p칰blico en estas zonas

3. **Optimizaci칩n de Tarifas Din치micas**:

   - Implementar tarifas variables en Manhattan basadas en la alta demanda y el alto revenue/km ($5.24)
   - Revisar pol칤tica de tarifas para viajes a aeropuertos (EWR) donde el revenue/km es bajo ($2.31)
   - Crear tarifas especiales para eventos anticipando el aumento de demanda

4. **Programa de Fidelizaci칩n por Zonas**:
   - Desarrollar incentivos para propinas en zonas donde son menos frecuentes (Staten Island: 12.8%)
   - Recompensar a conductores que cubren 치reas menos populares
   - Programa de lealtad para usuarios frecuentes en Manhattan (71.8% del mercado)

### Consideraciones T칠cnicas para Futuros An치lisis

1. **Escalabilidad de la Soluci칩n**:

   - El framework actual puede procesar eficientemente los 3M de registros mensuales
   - Para an치lisis en tiempo real, considerar tecnolog칤as de streaming como Kafka o Spark Streaming
   - Posibilidad de extender el pipeline para incluir modelos predictivos de demanda

2. **Integraci칩n de Fuentes Adicionales**:

   - Datos meteorol칩gicos: correlacionar clima con patrones de viaje
   - Datos demogr치ficos por zona: entender mejor el perfil de usuarios
   - Datos de congesti칩n de tr치fico: optimizar rutas y tiempos de viaje

3. **Dashboard en Tiempo Real**:

   - El pipeline de Prefect podr칤a alimentar un dashboard interactivo
   - Visualizaciones geoespaciales de densidad de viajes
   - Indicadores clave de rendimiento (KPIs) por zona y hora

4. **Limitaciones del An치lisis Actual**:
   - Solo incluye taxis amarillos (faltan taxis verdes y servicios de ridesharing)
   - Datos limitados a enero 2023 (falta an치lisis estacional)
   - No incorpora datos de satisfacci칩n de clientes o conductores

## Respuestas a Preguntas Clave

### 1. 쯈u칠 diferencia hay entre un LEFT JOIN y un INNER JOIN?

**INNER JOIN:**

- Conserva solo las filas donde existe coincidencia en ambas tablas
- Garantiza que todos los registros resultantes tengan informaci칩n completa de ambas tablas
- Puede reducir el tama침o del dataset eliminando registros sin coincidencias

**LEFT JOIN:**

- Conserva todas las filas de la tabla izquierda, aunque no haya coincidencia en la tabla derecha
- Cuando no encuentra coincidencia, las columnas de la derecha quedan como NULL
- 칔til cuando necesitamos preservar todos los registros de la tabla principal

### 2. 쯇or qu칠 usamos LEFT JOIN en lugar de INNER JOIN para trips+calendar?

Utilizamos LEFT JOIN para la combinaci칩n de trips_with_zones y calendar porque:

- Necesit치bamos mantener todos los viajes en el an치lisis, incluso los que ocurrieron en d칤as sin eventos especiales
- Un INNER JOIN habr칤a eliminado la mayor칤a de los viajes, ya que solo algunos d칤as ten칤an eventos especiales
- El objetivo era clasificar cada viaje como "d칤a normal" o "d칤a especial" sin perder ning칰n dato

### 3. 쯈u칠 problemas pueden surgir al hacer joins con datos de fechas?

Al trabajar con joins basados en fechas, enfrentamos varios desaf칤os:

- **Formatos inconsistentes**: Algunas fuentes almacenan fechas como strings, otras como datetime
- **Diferencia entre fecha y fecha-hora**: Tuvimos que extraer solo la parte de fecha para el join
- **Zonas horarias**: Diferentes datasets pueden usar diferentes zonas horarias
- **Granularidad**: Una fuente puede tener datos diarios mientras otra horarios

Para resolver estos problemas, implementamos la normalizaci칩n de formatos (trips['pickup_date'] = trips['tpep_pickup_datetime'].dt.date) antes de realizar los joins.

### 4. 쮺u치l es la ventaja de integrar m칰ltiples fuentes de datos?

La integraci칩n de m칰ltiples fuentes nos proporcion칩:

- **Contexto enriquecido**: No solo vimos los viajes, sino tambi칠n su dimensi칩n geogr치fica y temporal
- **Nuevas dimensiones anal칤ticas**: Pudimos analizar el impacto de eventos especiales y caracter칤sticas geogr치ficas
- **Insights m치s profundos**: Descubrimos patrones que ser칤an invisibles en datasets aislados
- **Visi칩n hol칤stica**: Comprendimos c칩mo diferentes factores (ubicaci칩n, eventos, hora) interact칰an entre s칤

### 5. 쯈u칠 insights de negocio obtuvimos del an치lisis integrado?

**Patrones por Zona:**

- Manhattan domina el mercado con 71.8% de los viajes y el mayor revenue/km
- Staten Island y Bronx muestran una severa brecha de cobertura (menos del 4% combinado)
- Las propinas var칤an significativamente por zona, siendo Manhattan la m치s generosa (18.2%)

**Impacto en Calendario:**

- Los d칤as con eventos especiales generan un 10.1% m치s de ingreso promedio por viaje
- Incremento del 7.2% en la distancia promedio recorrida durante eventos especiales

**Patrones Temporales:**

- La franja 5-7 PM concentra m치s de 124,000 viajes por hora
- Las horas valle (3-5 AM) tienen menos del 20% de la actividad de las horas pico

## Equipo

Este an치lisis fue desarrollado por el Grupo 1:

- **Joaqu칤n Batista**: Integraci칩n de datasets y optimizaci칩n
- **Milagros Cancela**: An치lisis geogr치fico y visualizaciones
- **Valent칤n Rodr칤guez**: An치lisis temporal y correlaciones
- **Alexia Aurrecoechea**: Interpretaci칩n de resultados y recomendaciones
- **Nahuel L칩pez**: Implementaci칩n de pipeline con Prefect
