---
title: "Escalado inteligente y pipelines anti-leakage: Optimizando modelos con preprocessing robusto"
description: "Exploraci√≥n avanzada de t√©cnicas de feature scaling y prevenci√≥n de data leakage en pipelines de machine learning usando el dataset Ames Housing."
---

[Jupyter Notebook original](../../public/images/practica6/Aurrecochea-Pr√°ctica6.ipynb)

## Objetivos de Aprendizaje

- **Identificar features problem√°ticas** que requieren escalado en datasets reales
- **Experimentar con diferentes scalers** (MinMaxScaler, StandardScaler, RobustScaler)
- **Descubrir el impacto del escalado** en algoritmos basados en distancia vs. tree-based
- **Comparar pipelines** con y sin data leakage para evidenciar diferencias
- **Desarrollar estrategias robustas** de preprocessing basadas en evidencia emp√≠rica

## Contexto de Negocio

El dataset **Ames Housing** presenta un desaf√≠o real de escalado: variables con rangos extremadamente diferentes que pueden sesgar algoritmos de machine learning. Como data scientists, debemos:

- **Identificar autom√°ticamente** features con escalas problem√°ticas
- **Seleccionar el scaler apropiado** seg√∫n caracter√≠sticas de los datos
- **Prevenir data leakage** en pipelines de producci√≥n
- **Evaluar el impacto** del escalado en diferentes tipos de algoritmos

### Relevancia del Problema

En modelos de predicci√≥n inmobiliaria, el escalado inadecuado puede:
- **Sesgar algoritmos basados en distancia** (KNN, SVM) hacia features con mayor escala
- **Afectar la convergencia** de algoritmos de optimizaci√≥n
- **Introducir data leakage** si se escala incorrectamente
- **Reducir la interpretabilidad** del modelo

## Proceso de An√°lisis

### 1. Configuraci√≥n del Entorno

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor  
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Configurar visualizaciones
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.size'] = 11
```

### 2. Exploraci√≥n de Escalas Problem√°ticas

```python
# Cargar dataset Ames Housing
df_raw = pd.read_csv('AmesHousing.csv')

# Identificar columnas num√©ricas
numeric_cols = df_raw.select_dtypes(include=[np.number]).columns.tolist()

# Seleccionar features con escalas problem√°ticas
selected_features = [
    'SalePrice', 'Lot Area', 'Overall Qual', 'Year Built', '1st Flr SF', 'Gr Liv Area'
]

# Analizar escalas
scale_analysis = {}
for col in selected_features:
    col_min = df_raw[col].min()
    col_max = df_raw[col].max()
    ratio = col_max / col_min if col_min != 0 else np.inf
    scale_analysis[col] = {
        'min': col_min,
        'max': col_max,
        'ratio': ratio
    }

scale_df = pd.DataFrame(scale_analysis).T
print("üìà An√°lisis de escalas:")
print(scale_df.sort_values(by='ratio', ascending=False))
```

### 3. Visualizaci√≥n del Problema de Escalas

```python
# Visualizar distribuciones
plt.figure(figsize=(16, 12))
for i, col in enumerate(selected_features):
    plt.subplot(3, len(selected_features)//3 + 1, i+1) 
    sns.boxplot(x=df_raw[col])
    plt.title(f'Boxplot de {col}')
    plt.xlabel(col)
    plt.grid(True)
plt.tight_layout()

# Histogramas con KDE
plt.figure(figsize=(16, 12))
for i, col in enumerate(selected_features):
    plt.subplot(3, len(selected_features)//3 + 1, i+1) 
    sns.histplot(df_raw[col], bins=30, kde=True)
    plt.title(f'Distribuci√≥n de {col}')
    plt.xlabel(col)
    plt.ylabel('Frecuencia')
    plt.grid(True)
plt.tight_layout()
```

![Histogramas de Distribuciones](/images/practica6/histogramas.png)

### 4. Preparaci√≥n de Datos y Split Anti-Leakage

```python
# Definir target y features
target_col = 'SalePrice'
feature_cols = ['Lot Area', 'Overall Qual', 'Year Built', '1st Flr SF', 'Gr Liv Area']

# Limpiar datos
df_clean = df_raw[feature_cols + [target_col]].dropna()

# Split ANTES de cualquier transformaci√≥n (cr√≠tico para evitar leakage)
X = df_clean[feature_cols]
y = df_clean[target_col]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.25, random_state=42
)

print(f"Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}")
```

### 5. Experimentaci√≥n con Diferentes Scalers

```python
# Configurar scalers
scalers = {
    'MinMax': MinMaxScaler(),
    'Standard': StandardScaler(),
    'Robust': RobustScaler(),
    'None': None
}

# Algoritmos a probar
algorithms = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'K-NN': KNeighborsRegressor(n_neighbors=5),
    'SVM': SVR(kernel='rbf', C=100)
}

# Experimentar combinaciones
results = {}
for scaler_name, scaler in scalers.items():
    for algo_name, algorithm in algorithms.items():
        if scaler is None:
            # Sin escalado
            pipeline = Pipeline([('model', algorithm)])
        else:
            # Con escalado
            pipeline = Pipeline([
                ('scaler', scaler),
                ('model', algorithm)
            ])
        
        # Entrenar y evaluar
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_val)
        
        r2 = r2_score(y_val, y_pred)
        mae = mean_absolute_error(y_val, y_pred)
        
        results[f"{scaler_name}_{algo_name}"] = {
            'R2': r2,
            'MAE': mae
        }

# Convertir a DataFrame para an√°lisis
results_df = pd.DataFrame(results).T
results_df[['Scaler', 'Algorithm']] = results_df.index.str.split('_', expand=True)
print("üìä Resultados de experimentaci√≥n:")
print(results_df.pivot(index='Algorithm', columns='Scaler', values='R2'))
```

### 6. An√°lisis de Data Leakage

```python
# Escenario CON leakage (INCORRECTO)
def pipeline_with_leakage(X_train, X_val, y_train, y_val):
    # ERRO: Escalar todo el dataset junto
    scaler = StandardScaler()
    X_all_scaled = scaler.fit_transform(pd.concat([X_train, X_val]))
    
    X_train_scaled = X_all_scaled[:len(X_train)]
    X_val_scaled = X_all_scaled[len(X_train):]
    
    model = LinearRegression()
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_val_scaled)
    
    return r2_score(y_val, y_pred)

# Escenario SIN leakage (CORRECTO)
def pipeline_without_leakage(X_train, X_val, y_train, y_val):
    # CORRECTO: Fit solo en train, transform en ambos
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    model = LinearRegression()
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_val_scaled)
    
    return r2_score(y_val, y_pred)

# Comparar resultados
r2_with_leakage = pipeline_with_leakage(X_train, X_val, y_train, y_val)
r2_without_leakage = pipeline_without_leakage(X_train, X_val, y_train, y_val)

print(f"R¬≤ CON leakage: {r2_with_leakage:.4f}")
print(f"R¬≤ SIN leakage: {r2_without_leakage:.4f}")
print(f"Diferencia: {r2_with_leakage - r2_without_leakage:.4f}")
```

### 7. Pipeline de Producci√≥n Robusto

```python
from sklearn.compose import ColumnTransformer

def create_production_pipeline(numeric_features, categorical_features=None):
    """Crear pipeline robusto para producci√≥n"""
    
    # Transformador num√©rico
    numeric_transformer = Pipeline(steps=[
        ('scaler', RobustScaler())  # Robusto a outliers
    ])
    
    # Transformador categ√≥rico (si aplicable)
    if categorical_features:
        from sklearn.preprocessing import OneHotEncoder
        categorical_transformer = Pipeline(steps=[
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ])
        
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numeric_features),
                ('cat', categorical_transformer, categorical_features)
            ]
        )
    else:
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numeric_features)
            ]
        )
    
    # Pipeline completo
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))
    ])
    
    return pipeline

# Crear y entrenar pipeline de producci√≥n
production_pipeline = create_production_pipeline(feature_cols)
production_pipeline.fit(X_train, y_train)

# Evaluar en conjunto de prueba
y_test_pred = production_pipeline.predict(X_test)
test_r2 = r2_score(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)

print(f"üìà Resultados en test set:")
print(f"R¬≤: {test_r2:.4f}")
print(f"MAE: ${test_mae:,.2f}")
```

### 8. Investigaci√≥n de Transformadores Avanzados

Para completar el an√°lisis, se explor√≥ el comportamiento de transformadores m√°s avanzados:

![Yeo-Johnson PowerTransformer](/images/practica6/YeoJohnson.png)

![QuantileTransformer](/images/practica6/QuantileTransformer.png)

![MaxAbsScaler](/images/practica6/MaxAbsScaler.png)

![Normalizer L2](/images/practica6/NormalizerL2.png)

![FunctionTransformer (Log)](/images/practica6/FunctionTransformer.png)

![Comparaci√≥n de Transformaci√≥n Logar√≠tmica](/images/practica6/comparacionlogs.png)

## Hallazgos Clave

### An√°lisis de Escalas

| Variable     | Rango (min‚Äìmax) | Ratio | ¬øProblem√°tica? | Raz√≥n |
|-------------|-----------------|-------|----------------|-------|
| Lot Area    | 1,300‚Äì215,245   | 165.6 | **S√≠**         | Rango extremo, outliers dominan |
| SalePrice   | 12,789‚Äì755,000  | 59.0  | **S√≠**         | Alta variabilidad afecta distancias |
| Overall Qual| 1‚Äì10            | 10.0  | No             | Escala controlada |

### Impacto por Algoritmo

1. **Algoritmos basados en distancia** (KNN, SVM): Mejora significativa con escalado
2. **Tree-based models** (Random Forest): Poco impacto del escalado
3. **Modelos lineales**: Beneficio moderado, especialmente con outliers

### Mejores Pr√°cticas Identificadas

1. **RobustScaler**: Mejor opci√≥n para datos con outliers
2. **StandardScaler**: Ideal para distribuciones normales
3. **MinMaxScaler**: √ötil cuando se necesita rango espec√≠fico [0,1]

## Consideraciones √âticas y de Calidad

### Prevenci√≥n de Data Leakage

- **Fit solo en training set**: Nunca ajustar transformaciones en validation/test
- **Pipeline autom√°tico**: Usar sklearn Pipeline para garantizar orden correcto
- **Validaci√≥n cruzada**: Aplicar escalado dentro de cada fold

### Transparencia del Modelo

- **Documentar decisiones**: Registrar por qu√© se eligi√≥ cada scaler
- **Monitorear drift**: Verificar si las escalas cambian en producci√≥n
- **Interpretabilidad**: Considerar impacto del escalado en explicabilidad

### Robustez en Producci√≥n

- **Manejo de outliers**: RobustScaler m√°s resistente a valores extremos
- **Datos faltantes**: Pipeline debe manejar NaN gracefully
- **Escalado inverso**: Capacidad de des-escalar predicciones si necesario

## Recursos Adicionales

### Enlaces √ötiles
- [Scikit-learn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)
- [Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/)
- [Kaggle Feature Scaling Guide](https://www.kaggle.com/learn/feature-engineering)

### Herramientas Utilizadas
- **scikit-learn**: Scalers y pipelines
- **pandas/numpy**: Manipulaci√≥n de datos
- **matplotlib/seaborn**: Visualizaciones
- **Pipeline**: Automatizaci√≥n anti-leakage

## Conclusi√≥n

El escalado de features es crucial para el rendimiento de muchos algoritmos de machine learning, pero debe implementarse cuidadosamente para evitar data leakage. La experimentaci√≥n sistem√°tica revel√≥ que RobustScaler ofrece la mejor combinaci√≥n de robustez y rendimiento para el dataset Ames Housing, especialmente en presencia de outliers. Los pipelines automatizados garantizan reproducibilidad y previenen errores comunes en producci√≥n.

---