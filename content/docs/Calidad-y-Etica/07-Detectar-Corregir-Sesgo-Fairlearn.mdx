---
title: "Detectando y corrigiendo sesgo algor√≠tmico: Fairlearn aplicado a casos reales de discriminaci√≥n"
description: "An√°lisis completo de sesgo en modelos de machine learning usando Fairlearn. Casos de estudio: Boston Housing (sesgo racial) y Titanic (sesgo de g√©nero/clase)."
---

[Jupyter Notebook original](../../public/images/practica7/Aurrecochea-Pr√°ctica7.ipynb)

## Objetivos de Aprendizaje

- **Detectar sesgo hist√≥rico** en datasets reales (Boston Housing, Titanic)
- **Analizar el impacto** del sesgo en predicciones de modelos
- **Comparar estrategias** de detecci√≥n vs. correcci√≥n autom√°tica
- **Evaluar cu√°ndo detectar** vs. cu√°ndo intentar corregir sesgo
- **Desarrollar criterios √©ticos** para deployment responsable de modelos

## Contexto de Negocio

El sesgo algor√≠tmico es uno de los problemas m√°s cr√≠ticos en machine learning moderno. Modelos aparentemente "objetivos" pueden perpetuar y amplificar discriminaci√≥n hist√≥rica, afectando decisiones en:

- **Pr√©stamos hipotecarios**: Discriminaci√≥n racial en valuaciones inmobiliarias
- **Seguros de vida**: Sesgos de g√©nero y clase social
- **Contrataci√≥n**: Perpetuaci√≥n de desigualdades hist√≥ricas
- **Sistema judicial**: Sesgos en predicci√≥n de reincidencia

### Relevancia del Problema

Los algoritmos entrenados con datos hist√≥ricos pueden:
- **Perpetuar discriminaci√≥n** sistem√°tica del pasado
- **Amplificar sesgos** existentes en los datos
- **Crear nuevas formas** de discriminaci√≥n indirecta
- **Afectar grupos vulnerables** de manera desproporcionada

## Proceso de An√°lisis

### 1. Configuraci√≥n del Entorno

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-learn
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, r2_score, mean_squared_error

# Fairlearn - Framework para equidad en ML
from fairlearn.metrics import (
    MetricFrame, 
    demographic_parity_difference, 
    equalized_odds_difference,
    selection_rate
)
from fairlearn.reductions import ExponentiatedGradient, DemographicParity

import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
np.random.seed(42)
```

## Parte I: Boston Housing - Detectar Sesgo Racial Hist√≥rico

### ¬øPor qu√© sklearn removi√≥ `load_boston()`?

El dataset Boston Housing fue removido de scikit-learn debido al **sesgo racial expl√≠cito** en la variable `B`, que representa la proporci√≥n de poblaci√≥n afroamericana por √°rea.

### 1. Cargar Dataset desde Fuente Original

```python
# Cargar desde fuente original (CMU)
data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep=r"\s+", skiprows=22, header=None, engine="python")

# Reconstruir formato especial del archivo
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

# Crear DataFrame
feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 
                'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']

boston_df = pd.DataFrame(data, columns=feature_names)
boston_df['MEDV'] = target

# Decodificar variable B problem√°tica
# B = 1000(Bk - 0.63)¬≤ ‚Üí Bk = sqrt(B/1000) + 0.63
boston_df['Bk_racial'] = np.sqrt(boston_df['B'] / 1000) + 0.63

print(f"‚úÖ Boston Housing cargado: {boston_df.shape}")
print(f"Variable racial Bk decodificada (proporci√≥n afroamericana)")
```

### 2. An√°lisis Exploratorio del Sesgo

```python
# Crear grupos raciales para an√°lisis
# Terciles de proporci√≥n afroamericana
boston_df['racial_group'] = pd.cut(
    boston_df['Bk_racial'], 
    bins=3, 
    labels=['Low_African_American', 'Medium_African_American', 'High_African_American']
)

# An√°lisis descriptivo por grupo racial
group_analysis = boston_df.groupby('racial_group').agg({
    'MEDV': ['mean', 'median', 'std'],
    'Bk_racial': ['mean', 'count'],
    'LSTAT': 'mean',  # % poblaci√≥n de bajo estatus socioecon√≥mico
    'CRIM': 'mean',   # Tasa de criminalidad
    'DIS': 'mean'     # Distancia a centros de empleo
}).round(2)

print("üìä An√°lisis por grupo racial:")
print(group_analysis)

# Visualizar distribuciones
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Precio medio por grupo racial
sns.boxplot(data=boston_df, x='racial_group', y='MEDV', ax=axes[0,0])
axes[0,0].set_title('Precio Medio de Vivienda por Grupo Racial')
axes[0,0].tick_params(axis='x', rotation=45)

# Correlaci√≥n entre variables problem√°ticas
correlation_vars = ['MEDV', 'Bk_racial', 'LSTAT', 'CRIM']
corr_matrix = boston_df[correlation_vars].corr()
sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, ax=axes[0,1])
axes[0,1].set_title('Matriz de Correlaci√≥n')

# Distribuci√≥n de variable racial
sns.histplot(data=boston_df, x='Bk_racial', bins=30, ax=axes[1,0])
axes[1,0].set_title('Distribuci√≥n de Proporci√≥n Afroamericana')

# Scatter plot precio vs. proporci√≥n racial
sns.scatterplot(data=boston_df, x='Bk_racial', y='MEDV', 
               hue='racial_group', alpha=0.7, ax=axes[1,1])
axes[1,1].set_title('Precio vs. Proporci√≥n Afroamericana')

plt.tight_layout()
plt.show()
```

![An√°lisis de Sesgo por Grupo Racial](/images/practica7/boxplot.png)

### 3. Cuantificar Sesgo en Predicciones

```python
# Entrenar modelo de regresi√≥n
X = boston_df[feature_names]
y = boston_df['MEDV']
sensitive_feature = boston_df['racial_group']

X_train, X_test, y_train, y_test, sens_train, sens_test = train_test_split(
    X, y, sensitive_feature, test_size=0.3, random_state=42
)

# Modelo base
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_pred = lr_model.predict(X_test)

# An√°lisis de equidad usando Fairlearn
metric_frame = MetricFrame(
    metrics={'mse': mean_squared_error, 'r2': r2_score},
    y_true=y_test,
    y_pred=y_pred,
    sensitive_features=sens_test
)

print("üìà M√©tricas por grupo racial:")
print(metric_frame.by_group)
print(f"\nDiferencia en MSE entre grupos: {metric_frame.by_group['mse'].max() - metric_frame.by_group['mse'].min():.3f}")
print(f"Diferencia en R¬≤ entre grupos: {metric_frame.by_group['r2'].max() - metric_frame.by_group['r2'].min():.3f}")

# An√°lisis de residuos por grupo
residuals = y_test - y_pred
residual_analysis = pd.DataFrame({
    'residuals': residuals,
    'racial_group': sens_test,
    'actual': y_test,
    'predicted': y_pred
})

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.boxplot(data=residual_analysis, x='racial_group', y='residuals')
plt.title('Distribuci√≥n de Residuos por Grupo Racial')
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
for group in residual_analysis['racial_group'].unique():
    group_data = residual_analysis[residual_analysis['racial_group'] == group]
    plt.scatter(group_data['actual'], group_data['predicted'], 
               label=group, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Valor Real')
plt.ylabel('Valor Predicho')
plt.title('Predicciones vs. Valores Reales por Grupo')
plt.legend()

plt.tight_layout()
plt.show()
```

## Parte II: Titanic - Detectar y Corregir Sesgo de G√©nero/Clase

### 1. Cargar y Preparar Dataset Titanic

```python
# Cargar Titanic dataset
titanic_url = "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"
titanic_df = pd.read_csv(titanic_url)

# Preparar datos
titanic_clean = titanic_df[['Survived', 'Pclass', 'Sex', 'Age', 'Fare']].dropna()

# Crear variable de interseccionalidad (g√©nero + clase)
titanic_clean['Gender_Class'] = titanic_clean['Sex'] + '_Class' + titanic_clean['Pclass'].astype(str)

# Preparar features para modelo
titanic_clean['Sex_encoded'] = titanic_clean['Sex'].map({'male': 0, 'female': 1})
X_titanic = titanic_clean[['Pclass', 'Sex_encoded', 'Age', 'Fare']]
y_titanic = titanic_clean['Survived']
sensitive_feature_titanic = titanic_clean['Sex']

print(f"‚úÖ Titanic dataset preparado: {titanic_clean.shape}")
print(f"Tasa de supervivencia general: {y_titanic.mean():.3f}")
```

### 2. An√°lisis de Sesgo Interseccional

```python
# An√°lisis de supervivencia por grupo
survival_analysis = titanic_clean.groupby(['Sex', 'Pclass']).agg({
    'Survived': ['count', 'mean', 'std']
}).round(3)

print("üìä An√°lisis de supervivencia por g√©nero y clase:")
print(survival_analysis)

# Visualizar sesgo interseccional
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Tasa de supervivencia por g√©nero
survival_by_gender = titanic_clean.groupby('Sex')['Survived'].mean()
survival_by_gender.plot(kind='bar', ax=axes[0,0])
axes[0,0].set_title('Tasa de Supervivencia por G√©nero')
axes[0,0].set_ylabel('Tasa de Supervivencia')

# Tasa de supervivencia por clase
survival_by_class = titanic_clean.groupby('Pclass')['Survived'].mean()
survival_by_class.plot(kind='bar', ax=axes[0,1])
axes[0,1].set_title('Tasa de Supervivencia por Clase')
axes[0,1].set_ylabel('Tasa de Supervivencia')

# Heatmap interseccional
pivot_survival = titanic_clean.pivot_table(
    values='Survived', index='Sex', columns='Pclass', aggfunc='mean'
)
sns.heatmap(pivot_survival, annot=True, cmap='RdYlBu', ax=axes[1,0])
axes[1,0].set_title('Tasa de Supervivencia: G√©nero √ó Clase')

# Distribuci√≥n por grupo interseccional
titanic_clean['Gender_Class'].value_counts().plot(kind='bar', ax=axes[1,1])
axes[1,1].set_title('Distribuci√≥n de Pasajeros por Grupo')
axes[1,1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()
```

### 3. Modelo Base y Detecci√≥n de Sesgo

```python
# Split de datos
X_train_t, X_test_t, y_train_t, y_test_t, sens_train_t, sens_test_t = train_test_split(
    X_titanic, y_titanic, sensitive_feature_titanic, test_size=0.3, random_state=42
)

# Modelo base (Random Forest)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_t, y_train_t)
y_pred_t = rf_model.predict(X_test_t)

# M√©tricas de equidad
metric_frame_titanic = MetricFrame(
    metrics={
        'accuracy': accuracy_score,
        'selection_rate': selection_rate
    },
    y_true=y_test_t,
    y_pred=y_pred_t,
    sensitive_features=sens_test_t
)

print("üìà M√©tricas del modelo base por g√©nero:")
print(metric_frame_titanic.by_group)

# Calcular m√©tricas de equidad espec√≠ficas
demo_parity_diff = demographic_parity_difference(
    y_test_t, y_pred_t, sensitive_features=sens_test_t
)
eq_odds_diff = equalized_odds_difference(
    y_test_t, y_pred_t, sensitive_features=sens_test_t
)

print(f"\n‚öñÔ∏è M√©tricas de Equidad:")
print(f"Diferencia en Paridad Demogr√°fica: {demo_parity_diff:.3f}")
print(f"Diferencia en Igualdad de Oportunidades: {eq_odds_diff:.3f}")
```

### 4. Correcci√≥n de Sesgo con Fairlearn

```python
# Aplicar correcci√≥n de sesgo
constraint = DemographicParity()
mitigator = ExponentiatedGradient(
    estimator=RandomForestClassifier(n_estimators=50, random_state=42),
    constraints=constraint
)

# Entrenar modelo corregido
mitigator.fit(X_train_t, y_train_t, sensitive_features=sens_train_t)
y_pred_fair = mitigator.predict(X_test_t)

# M√©tricas del modelo corregido
metric_frame_fair = MetricFrame(
    metrics={
        'accuracy': accuracy_score,
        'selection_rate': selection_rate
    },
    y_true=y_test_t,
    y_pred=y_pred_fair,
    sensitive_features=sens_test_t
)

# M√©tricas de equidad del modelo corregido
demo_parity_diff_fair = demographic_parity_difference(
    y_test_t, y_pred_fair, sensitive_features=sens_test_t
)
eq_odds_diff_fair = equalized_odds_difference(
    y_test_t, y_pred_fair, sensitive_features=sens_test_t
)

print("üìà M√©tricas del modelo corregido por g√©nero:")
print(metric_frame_fair.by_group)
print(f"\n‚öñÔ∏è M√©tricas de Equidad Corregidas:")
print(f"Diferencia en Paridad Demogr√°fica: {demo_parity_diff_fair:.3f}")
print(f"Diferencia en Igualdad de Oportunidades: {eq_odds_diff_fair:.3f}")

# Comparaci√≥n visual
comparison_df = pd.DataFrame({
    'Modelo': ['Base', 'Base', 'Corregido', 'Corregido'],
    'G√©nero': ['female', 'male', 'female', 'male'],
    'Accuracy': [
        metric_frame_titanic.by_group.loc['female', 'accuracy'],
        metric_frame_titanic.by_group.loc['male', 'accuracy'],
        metric_frame_fair.by_group.loc['female', 'accuracy'],
        metric_frame_fair.by_group.loc['male', 'accuracy']
    ],
    'Selection_Rate': [
        metric_frame_titanic.by_group.loc['female', 'selection_rate'],
        metric_frame_titanic.by_group.loc['male', 'selection_rate'],
        metric_frame_fair.by_group.loc['female', 'selection_rate'],
        metric_frame_fair.by_group.loc['male', 'selection_rate']
    ]
})

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

sns.barplot(data=comparison_df, x='Modelo', y='Accuracy', hue='G√©nero', ax=axes[0])
axes[0].set_title('Precisi√≥n por Modelo y G√©nero')

sns.barplot(data=comparison_df, x='Modelo', y='Selection_Rate', hue='G√©nero', ax=axes[1])
axes[1].set_title('Tasa de Selecci√≥n por Modelo y G√©nero')

plt.tight_layout()
plt.show()
```

## An√°lisis Cr√≠tico y Consideraciones √âticas

### Hallazgos Clave

#### Boston Housing
1. **Sesgo racial sistem√°tico**: √Åreas con mayor proporci√≥n afroamericana tienen valuaciones significativamente menores
2. **Correlaciones problem√°ticas**: Variables socioecon√≥micas correlacionadas con raza perpet√∫an discriminaci√≥n
3. **Impacto en predicciones**: Modelos reproducen y amplifican sesgos hist√≥ricos

#### Titanic
1. **Sesgo de g√©nero evidente**: Mujeres con 74% vs. hombres con 19% de supervivencia
2. **Interseccionalidad**: El sesgo se amplifica al combinar g√©nero y clase social
3. **Correcci√≥n parcial**: Fairlearn reduce pero no elimina completamente el sesgo

### Cu√°ndo Detectar vs. Corregir

#### Solo Detecci√≥n (Boston Housing)
- **Sesgo hist√≥rico profundo**: Datos reflejan discriminaci√≥n sistem√°tica
- **Contexto legal**: Usar raza expl√≠citamente es ilegal en muchos contextos
- **Transparencia**: Documentar y reportar sesgos sin intentar "corregir" autom√°ticamente

#### Detecci√≥n + Correcci√≥n (Titanic)
- **Sesgo contextual**: Protocolo "mujeres y ni√±os primero" puede ser aceptable
- **M√©tricas claras**: Objetivos de equidad bien definidos
- **Trade-offs aceptables**: Peque√±a p√©rdida de precisi√≥n por ganancia en equidad

### Criterios para Deployment Responsable

1. **Auditor√≠a continua**: Monitorear m√©tricas de equidad en producci√≥n
2. **Documentaci√≥n transparente**: Registrar decisiones y limitaciones
3. **Revisi√≥n humana**: Mantener supervisi√≥n humana en decisiones cr√≠ticas
4. **Actualizaci√≥n regular**: Re-entrenar modelos cuando cambien los datos

## Limitaciones y Trabajo Futuro

### Limitaciones Actuales
- **Definici√≥n de equidad**: M√∫ltiples definiciones pueden ser mutuamente excluyentes
- **Grupos protegidos**: Identificaci√≥n de todas las caracter√≠sticas sensibles
- **Sesgo de interseccionalidad**: Complejidad de m√∫ltiples caracter√≠sticas protegidas

### Investigaci√≥n Futura
- **M√©todos de correcci√≥n avanzados**: T√©cnicas que preserven mejor la utilidad
- **Detecci√≥n autom√°tica**: Identificaci√≥n de sesgos sin conocimiento previo
- **Evaluaci√≥n longitudinal**: Impacto de correcciones a largo plazo

## Recursos Adicionales

### Enlaces √ötiles
- [Fairlearn Documentation](https://fairlearn.org/)
- [Algorithmic Accountability Act](https://www.congress.gov/bill/117th-congress/house-bill/6580)
- [Google AI Principles](https://ai.google/principles/)
- [Partnership on AI Tenets](https://www.partnershiponai.org/tenets/)

### Herramientas Utilizadas
- **Fairlearn**: Framework principal para equidad en ML
- **scikit-learn**: Modelos base y m√©tricas
- **pandas/numpy**: Manipulaci√≥n de datos
- **matplotlib/seaborn**: Visualizaciones

## Conclusi√≥n

La detecci√≥n y correcci√≥n de sesgo algor√≠tmico requiere un enfoque multifac√©tico que combine an√°lisis t√©cnico riguroso con consideraciones √©ticas profundas. Los casos de Boston Housing y Titanic demuestran que no existe una soluci√≥n √∫nica: algunas situaciones requieren solo detecci√≥n y transparencia, mientras que otras permiten correcci√≥n autom√°tica. La clave est√° en desarrollar criterios claros para cada contexto y mantener la supervisi√≥n humana en decisiones cr√≠ticas.

El uso de herramientas como Fairlearn facilita la implementaci√≥n t√©cnica, pero la responsabilidad √©tica de deployment responsable recae en los equipos de data science y las organizaciones que implementan estos sistemas.

---